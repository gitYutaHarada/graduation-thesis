\chapter{予測モデルの構築}

本章では，前章で定義した「事故発生時情報モデル」と「事故後情報モデル」の構築手法について述べる．
死亡事故は全体の約 0.86\% と極めて稀な事象であり，強いクラス不均衡が存在する．
この課題に対処し，高い汎化性能と検知能力を両立させるため，本研究では異なる特性を持つ複数のアルゴリズムを組み合わせた多段階のアンサンブル学習プロセスを採用した．




\section{使用アルゴリズム}

本研究では，データの特性を多角的に捉えるため，決定木勾配ブースティングとニューラルネットワークという，学習原理の異なる複数のアルゴリズムを採用した．
また，極度のクラス不均衡（死亡事故率 < 1\%）に対処するため，各モデルの特性に合わせた詳細な調整を行っている．




\subsection{LightGBM}
LightGBM \cite{ke2017lightgbm} は，Microsoft Research の Guolin Ke らによって開発された，決定木ベースの勾配ブースティング手法である．
近年，データの大規模化に伴い，従来のGBDT（Gradient Boosting Decision Tree）の実装系では計算コストとメモリ消費量が大きな課題となっていた．
LightGBMは，これらの課題を解決し，大規模データに対しても高速かつ高精度な学習を可能にすることを目的として設計された．

\subsubsection{学習アルゴリズムの特性}
LightGBMは，連続値を「ビン（区間）」に区切ってヒストグラム化することで計算量を大幅に削減する．
また，決定木の成長において「Leaf-wise（損失が最も大きい葉を優先的に分割）」戦略を採用しており，予測誤差が大きいデータを集中的に学習することで，同じモデルサイズでもより高い精度を実現する．

\subsubsection{カテゴリ変数の処理}
本研究では，カテゴリ変数をOne-Hotエンコーディングするのではなく，LightGBMネイティブの \texttt{category} 型として扱っている．
LightGBMは，カテゴリ変数を目的変数の勾配統計量に基づいて並べ替え，ヒストグラム上で最適な分割点を探すアルゴリズムを内部で適用する．
これにより，都道府県や道路種別のような高カーディナリティ変数でも次元爆発を防ぎつつ，効率的な分岐学習が可能となる．

\subsubsection{不均衡データへの対策}
クラス不均衡への対策として，損失関数に \texttt{scale\_pos\_weight} を適用した．
正例（死亡事故）の重みを $\frac{\text{負例数}}{\text{正例数}}$ （約118倍）に設定することで，少数派である死亡事故の見逃しに対して大きなペナルティを与え，再現率を向上させている．




\subsection{CatBoost}
CatBoost \cite{prokhorenkova2018catboost} は，Yandex の Anna Veronika Dorogush らによって開発された，カテゴリ変数の扱いに特化したGBDTライブラリである．

\subsubsection{学習アルゴリズムの特性}
従来のブースティング手法で課題とされていた prediction shift（学習データとテストデータの分布の乖離）の問題を解決し，高精度かつ堅牢な予測を実現することを目的としている．
バイアスを除去したブースティング手法を採用しており，特にデータ数が少ない場合やカテゴリ変数が多い場合に高い汎化性能を発揮する．

\subsubsection{カテゴリ変数の処理}
CatBoostの最大の特徴は，Ordered TS (Target Statistics) と呼ばれる独自のアルゴリズムである．
カテゴリ変数を目的変数の平均値（事故率など）に変換する際，単純な平均ではなく，データをランダムに並べ替え，「自分より過去のデータ」のみを用いて計算を行う．
これにより，自身の正解ラベル情報が計算に含まれる「ターゲットリーク」を物理的に回避し，検証データに対して堅牢な特徴量を生成する．
本データセットには多数のカテゴリ変数が含まれることから，この機能は極めて有効である．

\subsubsection{不均衡データへの対策}
不均衡対策として，\texttt{auto\_class\_weights='Balanced'} を採用した．
これはクラスの出現頻度に反比例した重みを自動計算する機能であり，手動設定の手間を省きつつ，データ分布の変化に対してロバストな学習を可能にする．事前に固定の重みを設定した場合，データのフィルタリング強度が変わるたびにパラメータの再調整が必要となるが，本手法では入力データの分布に応じて常に最適なバランスが保たれる．
特にStage 2のTwo-Stageモデル（不均衡比が変化したデータセット）において，この自動調整機能が高い効果を発揮した．




\subsection{TabNet}
TabNet \cite{arik2021tabnet} は，Google Cloud AI の Sercan O. Arik らによって開発された，表形式データに特化したディープラーニングモデルである．
決定木が得意とする「特徴量選択を通じた解釈性」と，ニューラルネットワークが得意とする「表現学習能力」を統合し，前処理不要で高性能な学習を行うことを目指して設計された．

\subsubsection{学習アルゴリズムの特性}
TabNetは，人間の意思決定プロセスのように「見るべき特徴量を順番に選ぶ」仕組みを持つ．
全特徴量からそのステップで重要なものだけをスパース（疎）に選択し，部分的な判断を下すプロセスを繰り返すことで，不要なノイズ特徴量を無視しながら学習を進める．
また，どの特徴量が判断に使われたかをマスクとして可視化できるため，深層学習でありながら解釈性が高い．

\subsubsection{カテゴリ変数の処理}
カテゴリ変数の入力には，One-Hot化ではなく整数のIDへのマッピングを使用する．
ここでは \texttt{LabelEncoder} のような動的な探索を行わず，事前に構築した辞書を用いた高速かつ厳密なマッピングを採用した．
また，学習データに含まれない未知のカテゴリや欠損値に対しては，一律で \texttt{ID=0} (Unknown) を割り当てることで，テスト時のエラーを防いでいる．

\subsubsection{不均衡データへの対策}
TabNetのようなNNモデルにおいて，極端なクラス重み付けを行うと勾配が不安定になりやすい．
そのため，本研究では重み付けを行わず，代わりにバッチサイズを通常より小さく（例: 1024）設定する \texttt{Small Batch Strategy} を採用した．
学習の更新頻度を高めることで，少数派のシグナルを確率的に捉える機会を増やし，安定した収束を実現している．




\subsection{Multi-Layer Perceptron (MLP)}
MLP (Multi-Layer Perceptron) は，Rumelhart ら \cite{rumelhart1986learning} によって確立された誤差逆伝播法を学習基盤とする，最も基礎的かつ重要な深層学習モデルである．
変数の非線形な相互作用を学習する能力に長けており，アンサンブルモデルの一部として重要な役割を担う．

\subsubsection{学習アルゴリズムの特性}
決定木は特徴空間を垂直・水平に分割して階段状の境界線を作るが，MLPは滑らかな活性化関数により連続的な境界線を描く．
この決定境界の違いにより，決定木が「ルールに当てはまらない」として切り捨ててしまう微細なリスクを拾い上げ，アンサンブル全体の多様性を確保する役割を果たす．

\subsubsection{カテゴリ変数の処理}
MLPへの入力において，カテゴリ変数は以下の3ステップで厳密に処理している．
\begin{quote}
\begin{description}
\item[欠損値のシグナル化]\mbox{}\\
欠損値を単に埋めるのではなく，\texttt{\_missing} という明示的なカテゴリとして扱う仕組みを実装した．ただし，本実験のデータセットにおける欠損値は 0 であったため，実際にはこのカテゴリは使用されなかった．

\item[学習データのみでの定義]\mbox{}\\
学習データに存在しないカテゴリは「未知」と定義する．

\item[+1シフト]\mbox{}\\
未知のカテゴリを \texttt{0} にマッピングし，既知のカテゴリを \texttt{1} 以降にずらす．
\end{description}
\end{quote}
入力値が \texttt{0} の場合，Embedding層の重みとの積がゼロとなり，その特徴が無効化される（判断を保留する）．これにより，未知のデータに対しても数学的に安全な挙動が保証される．

\subsubsection{不均衡データへの対策}
損失関数には \texttt{BCEWithLogitsLoss} を採用した．
通常，モデルの出力を確率に変換する処理と誤差の計算は別段階で行われるが，本関数はこれらを内部でまとめて処理することで，数値計算上の誤差を抑え，学習を安定させる効果がある．
これに加え，\texttt{pos\_weight} 引数によって正例（死亡事故）の重みを大きく設定した．
これにより，少数派である死亡事故を誤って見逃した際に大きな損失（ペナルティ）が発生するようにし，モデルが死亡事故の検知を優先的に学習するよう調整している．




\subsection{Logistic Regression}
Logistic Regression（ロジスティック回帰）は，線形結合とシグモイド関数を用いた確率的分類モデルである．
本研究の最終段階におけるメタモデルとして採用した．
深層学習や勾配ブースティングと比較して構造が単純であるため，ベースモデルほどの表現力は持たないが，既に高度に加工された予測値を統合するタスクにおいては，その単純さが逆に強みとなる．

ロジスティック回帰は，1958年にD.R.Coxによって体系化された，二値分類問題に対する統計的な標準手法である．線形回帰を確率モデルへと拡張し，事象の発生確率を直接的かつ安定して推定するために開発された．

\subsubsection{学習アルゴリズムの特性}
ロジスティック回帰は，各特徴量（ベースモデルの予測確率など）に対して固有の係数（重み）を学習し，線形結合によって最終的な確率を算出する．
本研究で採用した主な理由は以下の2点である．
\begin{quote}
\begin{description}
\item[解釈性 (Interpretability)]\mbox{}\\
「どのモデルをどの程度信頼しているか」が係数の大小として直接的に可視化される．例えば，CatBoostの係数が大きければ，そのモデルを主軸として判断していることが一目で理解できる．これは，予測根拠の透明性が求められる交通事故予測において重要な特性である．

\item[過学習の回避]\mbox{}\\
メタモデルの入力特徴量は既に非線形変換を経た「予測確率」であり，データ数に対して次元数が少ない（約20次元）．このような状況で複雑なモデルを使用するとノイズを学習するリスクがあるが，線形モデルであればそのリスクを最小限に抑えられる．
\end{description}
\end{quote}

\subsubsection{正則化とパラメータ設定}
Stackingにおけるベースモデルの予測値間には，必然的に強い相関（多重共線性）が生じる．これに対処するため，本研究では \textbf{L2正則化} を適用した．
また，正則化強度パラメータ $C$ については，グリッドサーチの結果に基づき，標準的な $1.0$ よりも強力な正則化を意味する $C=0.01$ を採用した．
これにより，係数が極端な値になることを防ぎつつ，似たような予測能力を持つモデル間で重みを適切に分散させ，未知のデータに対する高い汎化性能を実現している．


\section{多段階学習戦略 (Multi-Stage Learning Strategy)}

不均衡データにおける「再現率」と「適合率」のトレードオフを解消するため，本研究では図\ref{fig:model_architecture}に示す3段階の学習ステージを構築した．

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{model_architecture.pdf}
  \caption{提案モデルの全体アーキテクチャ (3-Stage Stacking)}
  \label{fig:model_architecture}
\end{figure}




\subsection{Stage 1: 汎用モデルの構築とフィルタリング (Single-Stage Generalist)}

\subsubsection{目的と役割 (Objectives)}
本ステージでは，全学習データ（約130万件）を使用し，大局的な傾向を学習する「汎用モデル」を構築する．
このモデルは，全体像を把握して最終的な予測に寄与するだけでなく，後続のStage 2に向けて「明らかに安全な事例」を排除する\textbf{高性能なフィルタ}としての重要な役割を担う．




\subsubsection{アンサンブル最適化 (Ensemble Optimization via SLSQP)}
多様性を確保するため，性質の異なる4つのモデル（GBDT: LightGBM, CatBoost / NN: MLP, TabNet）を採用した．決定木モデルが得意とする「断片的なルールの発見」と，NNモデルが得意とする「滑らかな境界の学習」を相互に補完させるためである．
予測値の統合においては，単純平均ではなく，\textbf{SLSQP (Sequential Least Squares Programming)} アルゴリズムを用いた最適化を行った．
これは，学習に使用していない検証用データに対する予測値を用いて，PR-AUCを最大化するような重み配分を数理的に算出する手法である．重みの合計が $1.0$ になるという制約条件の下で探索を行い，確率としての整合性を保ちつつ，アンサンブルによる精度向上を最大化した．




\subsubsection{データ整合性のための固有ID (Unique ID Strategy)}
本システムでは，後続のStage 2においてデータの行削除（フィルタリング）が行われる．
そのため，単純な行番号管理では，最終的なStage 3での統合時にデータの対応関係が失われるという課題が生じる．
これに対処するため，初期段階からデータの固有ID（Original Index）を生成し，これを全ステージを通じて保持させる\textbf{固有ID}機構を導入した．
このシステム設計上の工夫により，フィルタリングによってデータ数が変化しても，最終段階で正確なマージが可能となる．

具体的な例として，データ $A, B, C$ の3件があり，それぞれの行番号が $0, 1, 2$ であるとする．
ここで中間の $B$ がフィルタリングにより削除されると，後ろにある $C$ の行番号は $2$ から $1$ に繰り上がり，元の行番号との対応関係が失われる．
この問題に対し，初期段階でそれぞれに不変の固有ID（$0, 1, 2$）を付与しておけば，データが削除されて位置が変わっても，このIDを参照することで $C$ を正しく特定し，正確に統合することが可能となる．

\subsubsection{Hard Sampleフィルタリング戦略 (Hard Sample Filtering Strategy)}
構築したStage 1モデルの予測結果を用いて，データの選別を行った．
閾値の決定においては，恣意的な値ではなく，「死亡事故の見逃しを最小限にする」という安全重視の基準に基づいて動的に決定した．
具体的には，PR曲線から算出された閾値 $0.0645$ を採用した．PR曲線は，不均衡データにおいて「見逃しの少なさ（再現率）」と「警報の正確さ（適合率）」のトレードオフ関係を可視化したものであり，本研究では「見逃しを最小限にする」ことを最優先して閾値を決定した．この閾値未満のデータを「Easy Sample」として除外した．
これにより，モデルが「自信を持って安全」と判断できる層を分離し，計算リソースを判断の難しい「Hard Sample」に集中させる戦略的プロセスを実現している．




\subsection{Stage 2: Hard Sample Miningに基づくTwo-Stage学習 (Two-Stage Specialist)}

\subsubsection{動機と戦略 (Motivation \& Strategy)}
極端な不均衡データ（1:118）において，汎用的なStage 1モデルでは，圧倒的多数の「自明な負傷事故」に学習リソースが割かれ，
境界付近の「死亡事故と紛らわしいケース」の識別が甘くなるという問題がある．
そこで本研究では，「全データを広く浅く学ぶStage 1（Generalist）」と，「判断が難しいデータのみを深く学ぶStage 2（Specialist）」に役割を分担するTwo-Stageアプローチを採用した．
Stage 2は，Stage 1が「安全（負傷）」と断定できなかったグレイゾーンの事例に対して，再審査を行う「矛」としての役割を担う．

\subsubsection{ハードサンプルの抽出プロセス (Hard Sample Extraction via Recall-Constraint)}
Stage 1（Ensemble）のOOF予測確率を利用してデータを絞り込んだ．
閾値の決定においては，単に確率で切るのではなく，\textbf{「Recall（再現率）98\%を維持する」}という制約条件下で閾値を動的に決定した（閾値 $0.0645$）．
死亡事故の見逃しは許されないため，十分な安全マージンを確保しつつ，確実に安全なデータだけを除外する設計とした．
このフィルタリングにより，学習データ数は約130万件から約55万件へと圧縮され，死亡事故含有率は約0.86\%から約2.04\%へと改善（濃縮）された．

\subsubsection{専門特化モデルの再構築 (Training of Specialized Models)}
絞り込んだ学習データに対し，Stage 1と同じ4つのアルゴリズムを用いてモデルを再構築した．
Stage 1では約130万件の全学習データを使用したが，Stage 2ではここから「Easy Sample」を除外し，残された約55万件の「Hard Samples」のみを独立した学習データセットとして再定義した．単純な続きの学習ではなく，全く異なるデータセットに対する新規の学習として扱うことで，特化モデルとしての性能を最大化している．

フィルタリングにより正例の割合が改善（$0.86\%$ $\to$ $2.04\%$）したことに伴い，データ分布の変化に対応するため各モデルのパラメータを厳密に再調整した．
具体的には，Stage 1では不均衡比（約1:118）に基づいて設定していた正例への重みを，Stage 2の新しい比率（約1:49）に合わせて緩和した．
LightGBMおよびMLPでは，損失関数の \texttt{pos\_weight} 引数を $118$ から $49$ へと変更し，過度なペナルティによる誤検知の増大を防いでいる．
一方，CatBoostでは \texttt{auto\_class\_weights='Balanced'} を継続して採用することで，変化したデータ分布に基づいて最適な重み係数を自動的に再計算させている．

また，モデルのハイパーパラメータだけでなく，前処理における統計量についても，Stage 1のものを流用せず，フィルタリング後のデータ群のみを用いて厳密に再計算を行った．
これにより，全データでは埋もれていた微細な差異が強調され，識別が困難な境界領域のデータに対して，より適切な特徴空間の正規化が適用されている．

特筆すべき点として，一般に決定木モデルは，データ量そのものよりも「境界の明確さ」が精度に寄与する特性がある．そのため，Easy Sampleの除外は，GBDTが計算リソースを境界付近の微細なパターン学習に集中させる上で有利に働くと期待される．対照的に，統計的な大数法則に依存するニューラルネットワークにとっては，学習データ数の減少が汎化性能の制約となる可能性がある．本研究のTwo-Stage戦略は，このアルゴリズム特性の違いを考慮して設計されている．

\subsubsection{統合と整合性の担保 (Integration \& Consistency)}
前述の固有ID機構を活用し，フィルタリング前後のデータ整合性を維持した．
Easy Sampleとしてフィルタリングされたデータ群に対しては，システムの整合性を保つために以下の処理パイプラインを実装した．
第一に，欠損となる予測スコアに対し，対応するStage 1の出力を補完値として適用した．
第二に，この補完処理が行われたレコードを識別するため，フラグ変数 \texttt{is\_easy\_sample}（除外=1, 再審査=0）を生成した．
この設計により，最終段のメタモデルは「Stage 2が高リスクと判断したスコア」と「Stage 1が低リスクと判断して確定したスコア」を明確に区別して学習できる．結果として，フラグ変数はモデル内で実質的な「安全保証バイアス」として機能し，誤検知の抑制に寄与している．




\subsection{Stage 3: スタッキング戦略 (Stacking Strategy)}

\subsubsection{概要と目的 (Overview)}
本研究の最終段階である Stage 3 の役割は，Stage 1（汎用モデル）と Stage 2（特化型モデル）の予測を統合し，最終的な予測精度を最大化することである．
Stage 2 のモデルは「難易度の高いデータ」のみで学習されているため，そのデータ分布は Stage 1（全量データ）とは大きく異なる．
これらの予測値を単純平均するだけでは，各モデルの得意・不得意や，データの文脈を十分に反映できないという課題がある．
そこで本研究では，メタモデル（ロジスティック回帰）を導入し，「どのような状況（場所・時間・不確実性）において，どのモデルの予測を信頼すべきか」を学習させるアプローチをとった．

\subsubsection{メタモデルのアーキテクチャ (Meta-Model Architecture)}
メタモデルとして，本研究ではロジスティック回帰を採用した．
当初は非線形な相互作用を捉えるためにLightGBMの採用も検討したが，予備実験においてLightGBMとロジスティック回帰の性能を比較した結果，両者の予測精度に有意な差は見られなかった．
性能が同等であれば，よりシンプルで解釈性の高いモデルが望ましい．ロジスティック回帰は，各ベースモデルの予測値に対する重み（係数）が明示的に得られるため，「どのモデルをどの程度信頼しているか」という判断根拠を説明しやすいという利点がある．
このため，最終的なメタモデルとして，L2正則化を適用したロジスティック回帰を選定した．

\subsubsection{特徴量エンジニアリング (Meta-Feature Engineering)}
Stacking の精度を最大化するため，本研究では単なる予測値の結合に留まらず，「どのような状況でモデル間の意見が食い違うか」を学習するための高度な特徴量エンジニアリングを実施した．
具体的には，不一致要因分析に基づき，以下の4カテゴリ・計20個以上の特徴量をメタモデルに入力した．

\begin{quote}
\begin{description}
\item[基本予測スコア (Base Predictions)]\mbox{}\\
Stage 1（全体観）と Stage 2（難問特化）の各モデルが出力した予測確率そのものである．これを判断の主軸として使用する．

\item[不一致と不確実性 (Uncertainty \& Diff)]\mbox{}\\
モデル間の予測値の標準偏差や，Stage 1 と Stage 2 の予測差分を算出・入力した．これらは「意見が割れている」という事実をリスク指標として利用するための重要なシグナルとなる．

\item[構造化フラグ (Structural Flags)]\mbox{}\\
データの性質を表すフラグ変数（\texttt{is\_easy\_sample}）である．このフラグが立っている場合，そのデータは Stage 2 モデルによる再審査を受けていない（補完値である）ことを意味し，メタモデルはその事実を考慮して最終判断を下す．

\item[コンテキスト特徴量 (Context Features)]\mbox{}\\
メタモデルに対し，\textbf{「モデル間の意見が食い違う（＝どのモデルを信じるべきか判断が難しい）条件」}を明示的に教えるための変数群である．具体的には，\texttt{month}（季節性）や \texttt{area\_id}（地域特性）などを注入することで，モデルが得意・不得意とする「状況」を提示している．これにより，メタモデルは単なる予測値の平均化ではなく，現在の環境下で最適なモデルを動的に判断する能力を獲得した．
\end{description}
\end{quote}

\paragraph{コンテキスト特徴量の選定プロセス}
メタモデルにどのコンテキスト情報を教示すべきかを決定するため，「モデル間の予測が食い違う条件」を特定する統計的検定を実施した．分析アプローチは以下の通りである．

\begin{quote}
\begin{description}
\item[不一致度の定義]\mbox{}\\
Stage 1（全体モデル）とStage 2（特化モデル）の予測値の絶対差 $|P_{stage1} - P_{stage2}|$ を計算し，不一致度とした．

\item[統計的検定]\mbox{}\\
不一致度が高い群と低い群の間で，各特徴量の分布に有意差があるかを検定した．カテゴリ変数にはカイ二乗検定，数値変数にはマン・ホイットニーのU検定を適用した．

\item[フィルタリング]\mbox{}\\
Benjamini-Hochberg法による多重検定補正を行い，補正後$p$値 $< 0.05$ の有意な特徴量のみを採用した．
\end{description}
\end{quote}

最終的に，これらの基準を満たし，かつ上位八つの特徴量を採用した．
本研究では「事故発生時情報モデル」と「事故後情報モデル」で利用可能な情報が異なるため，それぞれに対して個別に選定を行った．

まず，直ちに得られる情報のみを使用する\textbf{事故発生時情報モデル}においては，主に環境要因や位置情報が選定された（表\ref{tab:context_immediate}）．

\begin{table}[htbp]
  \centering
  \caption{事故発生時情報モデルにおけるコンテキスト特徴量}
  \label{tab:context_immediate}
  \begin{tabular}{lll}
    \toprule
    特徴量 & 分類 & 選定理由（役割・解釈） \\
    \midrule
    \textbf{year} & 経年傾向 & 年によって事故パターンの難易度が異なる \\
    \textbf{month} & 季節性 & 月ごとの環境変化がモデルの判断に影響 \\
    \textbf{都道府県コード} & 広域地域性 & 都市部/地方部でのモデル性能差 \\
    \textbf{area\_id} & 詳細地域性 & メッシュレベルでの局所的な特性 \\
    \textbf{昼夜} & 視認性 & 昼と夜で信頼すべきモデルが変わる \\
    \textbf{一時停止規制 表示 (A)} & 交通規制 & 当事者A（主原因側）の一時停止有無 \\
    \textbf{一時停止規制 表示 (B)} & 交通規制 & 当事者B（相手方）の一時停止有無 \\
    \textbf{速度規制 (指定のみ) (B)} & 速度環境 & 速度域によるリスク評価の食い違い \\
    \bottomrule
  \end{tabular}
\end{table}

一方，詳細調査後の情報も利用可能な\textbf{事故後情報モデル}においては，相手方の属性や詳細な衝突状況など，よりミクロな要因が強く影響していることが判明した（表\ref{tab:context_post_accident}）．

\begin{table}[htbp]
  \centering
  \caption{事故後情報モデルにおけるコンテキスト特徴量}
  \label{tab:context_post_accident}
  \begin{tabular}{lcl}
    \toprule
    特徴量 & 分類 & 選定理由（効果量と解釈） \\
    \midrule
    \textbf{当事者種別} & 相手方属性 & 予測難易度への影響が大 ($r=0.154$) \\
    \textbf{車両の衝突部位} & 状況詳細 & 衝突形態が過失判断に直結 ($r=0.141$) \\
    \textbf{geohash\_accidents\_past\_365d} & 事故履歴 & 事故多発地点か否かが確信度に影響 \\
    \textbf{一時停止規制 標識} & 交通環境 & 交差点の複雑さを示す指標 \\
    \textbf{用途別} & 相手方属性 & 事業用車両などの挙動特性差 \\
    \textbf{traffic\_24h} & 交通量 & 交通状況の複雑さ \\
    \textbf{peak\_ratio} & 交通量 & 混雑時の判断割れ要因 \\
    \bottomrule
  \end{tabular}
\end{table}

この分析により，当初想定していたマクロな環境要因（年，月，都道府県など）よりも，相手方の属性や詳細な衝突状況といったミクロな要因の方が，モデル間の判断不一致に強く影響していることが判明した．これらを追加することで，メタモデルはより具体的な状況に基づいて信頼度を調整可能となった．

\subsubsection{異種データの統合 (Heterogeneous Data Integration)}
異なるレコード数を持つ Stage 1（全量）と Stage 2（部分集合）の予測値を統合するため，前節までに述べた固有ID（\texttt{original\_index}，第4.2.1項参照）および Easy Sample 補完フラグ（\texttt{is\_easy\_sample}，第4.2.2項参照）を活用した．
具体的には，固有IDをキーとして全データを結合し，Stage 2 の欠損値には Stage 1 の予測値を代入した上でフラグを付与することで，データの不整合を解消しつつ，メタモデルが情報の出所（補完か否か）を識別できる構造とした．

